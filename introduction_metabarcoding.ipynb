{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02597e6-6f87-4c05-83fb-57a0573c9ba9",
   "metadata": {},
   "source": [
    "# Introduction to metabarcoding: DADA2 Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d41650-2eda-49f2-904f-92d07ce9060b",
   "metadata": {},
   "source": [
    "**Workshop led by: (Pedro E. Romero)** <br>\n",
    "**Assisted by: Boris ..., Camila Castillo-Vilcahuaman** <br>\n",
    "**Tutorial by: @reymonera (Camila Castillo-Vilcahuaman)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15815d-1022-4177-9e77-482d00d74f00",
   "metadata": {},
   "source": [
    "This is a tutorial with all the necessary commands for the use of the R package DADA2. All the analysis are done and exposed here with their respective figures. In this opportunity, we will use data from an already published study: Metabarcoding in the most important river in Lima, Peru: The Rimac river.\n",
    "\n",
    "This pipeline and tutorial is heavily based on another great bioinformatician's pipeline (Hi, @AstroBioMike!). If you want to learn more about this package, I highly suggest to visit this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0785f618-f112-48d5-8c1d-d1f881e12a38",
   "metadata": {},
   "source": [
    "## 1. Getting to know the DADA2 package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe4edd-d9db-4e53-ae8e-125c1c9e9af2",
   "metadata": {},
   "source": [
    "First of all, we will get a first glance of how this environment stuff works. First of all, this is a Jupyter notebook. It is easier to show you guys how code works using these.\n",
    "However, for our workshop to work well, we will need to set everything up first. So, for this, we will proceed with the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187c7fc-90ec-4bc8-9bbd-c44e156da91f",
   "metadata": {},
   "source": [
    "```\n",
    "conda env create -f dada2_env.yaml\n",
    "conda activate dada2_env\n",
    "python -m ipykernel install --user --name=dada2_env\n",
    "jupyter-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70090723-cbe4-4a55-9c1b-3d94596aa508",
   "metadata": {},
   "source": [
    "Now, we will proceed with enabling R code in this notebook. As you've probably guessed, Jupyer mainly runs with python. But we can make our own stuff run here using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bcbe8e-8484-40c6-b8ab-00edd30537d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marlen/miniforge3/envs/dada2_env/lib/python3.13/site-packages/rpy2/ipython/rmagic.py:85: UserWarning: The Python package `pandas` is strongly recommended when using `rpy2.ipython`. Unfortunately it could not be loaded, as we did not manage to load `numpy` in the first place (error: No module named 'numpy').\n",
      "  warnings.warn('The Python package `pandas` is strongly '\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29359974",
   "metadata": {},
   "source": [
    "## 2. Getting to know our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c95382-1848-49f6-8c55-8b076fb5ae29",
   "metadata": {},
   "source": [
    "Empezamos por descargar la información necesaria para nuestro análisis. Estaremos trabajando con la data del estudio de las comunidades microbianas presentes en el río Rímac. Para ello requeriremos descargarlas de la siguiente forma: \n",
    "- Primero, nos dirigimos a la pestaña “Terminal” de nuestro RStudio. Luego, pegaremos los siguientes comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993b06c-0391-4e07-bf0c-c2564cdce92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~\n",
    "!curl -L -o dada2_amplicon_ex_workflow.tar.gz\n",
    "!https://ndownloader.figshare.com/files/28773936\n",
    "!tar -xzvf dada2_amplicon_ex_workflow.tar.gz\n",
    "!rm dada2_amplicon_ex_workflow.tar.gz\n",
    "!cd dada2_amplicon_ex_workflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031665c-95b7-4cd2-a88a-b0ca6d093c6f",
   "metadata": {},
   "source": [
    "Con el último comando, nos podremos ubicar dentro de la carpeta. Si le damos a `ls` podremos observar los nuevos archivos que hemos adquirido.\n",
    "\n",
    "- Ahora, requeriremos un listado de los archivos que tiene por nombre `samples` con la información de nombres de cada uno de los archivos. Esto hará más simple el moverse por los archivos, y facilitará varios aspectos relacionados al procesamiento. Estaremos llamando a estos archivos constantemente. Quedándonos en “Terminal”, ejecutamos el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f7567-d2a3-4c9d-9471-85252cbed034",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *_R1.fq | cut -f1 -d \"_\" > samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4644a-bc32-4c2c-98e8-bc105f7d8431",
   "metadata": {},
   "source": [
    "- Toca hacerle un tratamiento a nuestros archivos de reads. Ello implica cortar los adaptadores con los que vienen por default. Eso lo podemos lograr con `cutadapt`. En nuestra terminal, ejecutamos el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8919035-2ce6-4fd8-af94-972771577b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cutadapt --version # 2.3\n",
    "!cutadapt -a ^GTGCCAGCMGCCGCGGTAA...ATTAGAWACCCBDGTAGTCC \\\n",
    "-A ^GGACTACHVGGGTWTCTAAT...TTACCGCGGCKGCTGGCAC \\\n",
    "-m 215 -M 285 --discard-untrimmed \\\n",
    "-o B1_sub_R1_trimmed.fq -p B1_sub_R2_trimmed.fq \\\n",
    "B1_sub_R1.fq B1_sub_R2.fq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1770c6-3f09-43c0-99f3-f0f1a721074d",
   "metadata": {},
   "source": [
    "Y ejecutaremos `cutadapt` para B1. Si deseamos ejecutar cutadapt para el resto de nuestras\n",
    "muestras, lo mejor será utilizar un artilugio del lenguaje Bash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455228d-5715-4b22-8ebe-963e685570d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!for sample in $(cat samples)\n",
    "!do\n",
    "    !echo \"On sample: $sample\"\n",
    "\n",
    "    !cutadapt -a ^GTGCCAGCMGCCGCGGTAA...ATTAGAWACCCBDGTAGTCC \\\n",
    "    -A ^GGACTACHVGGGTWTCTAAT...TTACCGCGGCKGCTGGCAC \\\n",
    "    -m 215 -M 285 --discard-untrimmed \\\n",
    "    -o ${sample}_sub_R1_trimmed.fq.gz -p ${sample}_sub_R2_trimmed.fq.gz \\\n",
    "    ${sample}_sub_R1.fq ${sample}_sub_R2.fq \\\n",
    "    >> cutadapt_primer_trimming_stats.txt 2>&1\n",
    "!done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b998b-2db6-4416-af73-18851ca02cda",
   "metadata": {},
   "source": [
    "Esto terminará por realizar el tratamiento en nuestra data. Ahora sí, podremos dirigirnos a la consola de R y empezar a poner todo en orden. Primero, señalamos en donde nos vamos a quedar y cuál será nuestro espacio de trabajo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb3292",
   "metadata": {},
   "source": [
    "## 3. Starting with DADA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de71057-0b94-410f-9ff6-5704164e197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"/home/marlen/gitrepos/dada2_msm-course-2024\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "getwd()\n",
    "setwd(\"~/dada2_amplicon_ex_workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41222cba-0342-4b85-8057-e9dd494200a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ‘1.30.0’\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(dada2)\n",
    "packageVersion(\"dada2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92916e-ae7a-4dda-b3e0-33aea5f76d15",
   "metadata": {},
   "source": [
    "Crea una variable llamada `samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e162cd-2e2c-4f4c-8306-958a4422104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "samples <- scan(\"samples\", what=\"character\")\n",
    "list.files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fe116",
   "metadata": {},
   "source": [
    "Crea dos variables para los archivos filtrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Iremos creando las variables para conservar los reads filtrados\n",
    "filtered_forward_reads <- paste0(samples, \"_sub_R1_filtered.fq.gz\")\n",
    "filtered_reverse_reads <- paste0(samples, \"_sub_R2_filtered.fq.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce192f2c",
   "metadata": {},
   "source": [
    "Crea dos variables `lecturas_directas(forward_reads)` y `lecturas_reversas(reverse_reads)` añadiendo etiquetas _R1 o _R2 a la variable muestras, de modo que concuerde con los nombres de archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae740a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "forward_reads <- paste0(samples, \"_sub_R1_trimmed.fq.gz\")\n",
    "reverse_reads <- paste0(samples, \"_sub_R2_trimmed.fq.gz\")\n",
    "\n",
    "#Iremos creando las variables en donde entrarán los reads ya “podados”\n",
    "trimmed_forward_reads <- paste0(samples, \"_1_trimmed.fastq\")\n",
    "trimmed_reverse_reads <- paste0(samples, \"_2_trimmed.fastq\")\n",
    "\n",
    "#A matrix with no blanks included\n",
    "trimmed_forward_reads[1:25] -> trimmed_forward_reads_noblanks\n",
    "trimmed_reverse_reads[1:25] -> trimmed_reverse_reads_noblanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a105680",
   "metadata": {},
   "source": [
    "### Quality Plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825c666",
   "metadata": {},
   "source": [
    "Para saber en donde vamos a cortar, necesitaremos primero ver gráficos de calidad. Eso lo lograremos mediante el comando que le solicita a DADA2 un perfil de calidad, mediante los siguientes comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23991a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "plotQualityProfile(forward_reads)\n",
    "plotQualityProfile(reverse_reads)\n",
    "plotQualityProfile(reverse_reads[17:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b9742",
   "metadata": {},
   "source": [
    "Revisemos el gráfico de calidad, la longitud de lectura es 250 nt. Observemos más o menos en qué punto es que se da la caída en calidad en nuestra data. Esto nos ayudará a los parámetros a continuación.\n",
    "\n",
    "Luego, utilizaremos el comando `filterAndTrim` que nos permite editar los reads de acuerdo a nuestras necesidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed49ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "filtered_out <- filterAndTrim(forward_reads, filtered_forward_reads,\n",
    "reverse_reads, filtered_reverse_reads, maxEE=c(2,2),\n",
    "rm.phix=TRUE, minLen=175, truncLen=c(250,200))\n",
    "\n",
    "filtered_out\n",
    "\n",
    "#Podemos explorar nuestra data con:\n",
    "class(filtered_out)\n",
    "dim(filtered_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccabe3b",
   "metadata": {},
   "source": [
    "Vale la pena ahora chequear si es que nuestro filtro realmente valió la pena o no. Para ello, podemos volver a dibujar la data que tenemos disponible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed20727",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "plotQualityProfile(filtered_forward_reads)\n",
    "plotQualityProfile(filtered_reverse_reads)\n",
    "plotQualityProfile(filtered_reverse_reads[17:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7bafc",
   "metadata": {},
   "source": [
    "¿Tuvo algún efecto lo cortado y filtrado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8645582e",
   "metadata": {},
   "source": [
    "### Learning from our errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94254b",
   "metadata": {},
   "source": [
    "Todo secuenciamiento no está libre de errores. Y DADA2 desea conocerlos. Este es un paso únicamente necesario para nuestro paquete. De aquí la información que nos interesa es poca, pero de todos modos es un paso importante (y computacionalmente intensivo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf86812",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "err_forward_reads <- learnErrors(filtered_forward_reads)\n",
    "err_reverse_reads <- learnErrors(filtered_reverse_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0bdba2",
   "metadata": {},
   "source": [
    "Finalmente procedemos a dibujar nuestras gráficas de error de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50889104",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "plotErrors(err_forward_reads, nominalQ=TRUE)\n",
    "plotErrors(err_reverse_reads, nominalQ=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c793d5b",
   "metadata": {},
   "source": [
    "Y sale... esto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44b168",
   "metadata": {},
   "source": [
    "Ahm... no hay mucho que hacer por aquí. Nuevamente, aprender los errores es algo que se necesitará en comandos posteriores. En nuestro caso, lo único importante es que los puntos negros sigan a la curva negra. Eso nos indica la asociación entre observado y estimado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811eed92",
   "metadata": {},
   "source": [
    "### Derreplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ccf765",
   "metadata": {},
   "source": [
    "¿Queremos gastar nuestros recursos tratando de determinar a trillones de secuencias idénticas?\n",
    "\n",
    "¿No sería mejor poner la secuencia e indicar la cantidad de veces que anda por ahí? \n",
    "\n",
    "Ok, entonces utilizaremos el comando que nos permite ello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "derep_forward <- derepFastq(filtered_forward_reads, verbose=TRUE)\n",
    "names(derep_forward) <- samples\n",
    "\n",
    "derep_reverse <- derepFastq(filtered_reverse_reads, verbose=TRUE)\n",
    "names(derep_reverse) <- samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ff8b0",
   "metadata": {},
   "source": [
    "### Inferring ASVs with the DADA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f86e3",
   "metadata": {},
   "source": [
    "Ahora sí, pasemos a los ASVs. Habíamos hablado mucho de estos, pero ya era hora de utilizar un comando para poder tenerlos en nuestras pantallas. Así es cómo utilizamos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51652baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "dada_forward <- dada(derep_forward, err=err_forward_reads,\n",
    "pool=\"pseudo\")\n",
    "\n",
    "dada_reverse <- dada(derep_reverse, err=err_reverse_reads,\n",
    "pool=\"pseudo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699d95b",
   "metadata": {},
   "source": [
    "El último parámetro `pool` es difícil de explicar, pero podemos decir que las dos opciones eran o chequear muestra por muestra o coger todas las muestras en lo que se conoce como “pooling”. Sin embargo, esto podía terminar obviando algunas secuencias que se perderían en la “piscina” de opciones. \n",
    "\n",
    "Entonces, se creó el *pseudo-pooling*. A continuación, uniremos a las secuencias forward y reverse, ya que esto será nuestra secuencia\n",
    "blanco para poder realizar la identificación de las mismas. Es lo lograremos con los siguientes comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "merged_amplicons <- mergePairs(dada_forward, derep_forward,\n",
    "dada_reverse, derep_reverse, trimOverhang=TRUE, minOverlap=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d8eb1",
   "metadata": {},
   "source": [
    "Podemos explorar un poco esta variable mediante estos comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95da9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "class(merged_amplicons)\n",
    "length(merged_amplicons)\n",
    "names(merged_amplicons)\n",
    "\n",
    "class(merged_amplicons$B1)\n",
    "names(merged_amplicons$B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f157d",
   "metadata": {},
   "source": [
    "### ASVs count table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae2929",
   "metadata": {},
   "source": [
    "Quizá el output más importante sea la tabla que se obtiene, a la que mundanamente se le conoce como la tabla de los counts. Esta la podemos producir de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c969cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "seqtab <- makeSequenceTable(merged_amplicons)\n",
    "class(seqtab) # matrix\n",
    "dim(seqtab) # 20 2521"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df93faf",
   "metadata": {},
   "source": [
    "### Chimera identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a98dc0",
   "metadata": {},
   "source": [
    "Por último, identificaremos a las quimeras. Estas son secuencias que se pegan sin ningún significado biológico concreto. DADA2 realiza esta identificación haciendo estimaciones entre las secuencias más abundantes y las menos, tratando de inferir si es que se pudieron juntar de alguna forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc19178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "seqtab.nochim <- removeBimeraDenovo(seqtab, verbose=T)\n",
    "\n",
    "#Y podemos revisar cuánto hemos perdido con esto también. La idea es\n",
    "que tengamos un decimal cercano al 1:\n",
    "sum(seqtab.nochim)/sum(seqtab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37123fa",
   "metadata": {},
   "source": [
    "Finalmente, para finalizar este paso, es adecuado tener una tabla resumen de todo lo que hemos realizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d95cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "getN <- function(x) sum(getUniques(x))\n",
    "summary_tab <- data.frame(row.names=samples,\n",
    "dada2_input=filtered_out[,1], filtered=filtered_out[,2],\n",
    "dada_f=sapply(dada_forward, getN), dada_r=sapply(dada_reverse, getN),\n",
    "merged=sapply(merged_amplicons, getN), nonchim=rowSums(seqtab.nochim),\n",
    "final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]\n",
    "*100, 1))\n",
    "\n",
    "summary_tab\n",
    "\n",
    "write.table(summary_tab, \"read-count-tracking.tsv\", quote=FALSE,\n",
    "sep=\"\\t\", col.names=NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c228c",
   "metadata": {},
   "source": [
    "## 4. Assigning taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5073a",
   "metadata": {},
   "source": [
    "Normalmente, para poder realizar este paso requeriríamos la descarga la base de datos 16S rRNA [SILVA] (https://zenodo.org/record/3731176#.YJa8K-hKg2w). Sin embargo, las capacidades nos limitan un poco al respecto de esto. Lo mejor que puedo hacer es dejar un código, el cuál es mejor no ejecutar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "## skipping this codeblock for time, and it will not run in the binder environment\n",
    "## downloading DECIPHER-formatted SILVA v138 reference\n",
    "#download.file(url=\"http://www2.decipher.codes/Classification/TrainingSets/SILVA_SSU_r138_2019.RData\", destfile=\"SILVA_SSU_r138_2019.RData\")\n",
    "\n",
    "## loading reference taxonomy object\n",
    "# load(\"SILVA_SSU_r138_2019.RData\")\n",
    "## loading DECIPHER\n",
    "# library(DECIPHER)\n",
    "# packageVersion(\"DECIPHER\") # v2.6.0 when this was initially put together, though might be different in the binder or conda installation, that's ok!\n",
    "\n",
    "## creating DNAStringSet object of our ASVs\n",
    "# dna <- DNAStringSet(getSequences(seqtab.nochim))\n",
    "## and classifying\n",
    "# tax_info <- IdTaxa(test=dna, trainingSet=trainingSet, strand=\"both\", processors=NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e93ba8",
   "metadata": {},
   "source": [
    "Como no tenemos estas capacidades, utilizaremos lo siguiente, que es data ya estimada por el autor y que nos permite acceder a este análisis desde ya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "load(\"tax-info.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48199b8",
   "metadata": {},
   "source": [
    "### Taxa-information tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd03bc",
   "metadata": {},
   "source": [
    "Normalmente en este tipo de estudios requerimos tablas de secuencias, conteo y taxonomía. Son formatos estándares que son posibles de exportar a otros paquetes. Esto lo podemos conseguir con los siguientes comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d644b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# asignando nuevos nombres (ASV_1, ASV_2...)\n",
    "asv_seqs <- colnames(seqtab.nochim)\n",
    "asv_headers <- vector(dim(seqtab.nochim)[2], mode=\"character\")\n",
    "\n",
    "for (i in 1:dim(seqtab.nochim)[2]) {\n",
    "asv_headers[i] <- paste(\">ASV\", i, sep=\"_\")\n",
    "}\n",
    "\n",
    "# escribiendo fastas de nuestros ASVs\n",
    "asv_fasta <- c(rbind(asv_headers, asv_seqs))\n",
    "write(asv_fasta, \"ASVs.fa\")\n",
    "\n",
    "# Tabla de conteo:\n",
    "asv_tab <- t(seqtab.nochim)\n",
    "row.names(asv_tab) <- sub(\">\", \"\", asv_headers)\n",
    "write.table(asv_tab, \"ASVs_counts.tsv\", sep=\"\\t\", quote=F, col.names=NA)\n",
    "\n",
    "# Tabla de taxas:\n",
    "# Crear tabla de taxonomías y diciendo que los que no existen son NA\n",
    "ranks <- c(\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\")\n",
    "\n",
    "asv_tax <- t(sapply(tax_info, function(x) {\n",
    "    m <- match(ranks, x$rank)\n",
    "    taxa <- x$taxon[m]\n",
    "    taxa[startsWith(taxa, \"unclassified_\")] <- NA\n",
    "    taxa\n",
    "}))\n",
    "\n",
    "colnames(asv_tax) <- ranks\n",
    "rownames(asv_tax) <- gsub(pattern=\">\", replacement=\"\", x=asv_headers)\n",
    "\n",
    "write.table(asv_tax, \"ASVs_taxonomy.tsv\", sep = \"\\t\", quote=F, col.names=NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c1781",
   "metadata": {},
   "source": [
    "### Identifying contaminants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01ea3b",
   "metadata": {},
   "source": [
    "Algo muy importante en este tipo de estudios es tener blancos. Muestras sin nada que nos puedan decir si es que hay contaminantes en nuestras propias muestras. De este modo, si especie 1 se encontró en el blanco, es un trabajo más simple eliminar a especie 1 de las muestras. Este descarte lo podemos hacer con el siguiente paquete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(decontam)\n",
    "packageVersion(\"decontam\")\n",
    "\n",
    "colnames(asv_tab) # en estas muestras, los blancos son los primeros 4\n",
    "\n",
    "vector_for_decontam <- c(rep(TRUE, 4), rep(FALSE, 16))\n",
    "contam_df <- isContaminant(t(asv_tab), neg=vector_for_decontam)\n",
    "table(contam_df$contaminant)\n",
    "\n",
    "contam_asvs <- row.names(contam_df[contam_df$contaminant == TRUE, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19079ea",
   "metadata": {},
   "source": [
    "Ajá, identificados. Ahora, a removerlos y realizar conteos sin contaminantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79907ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# haciendo un nuevo archivo fasta\n",
    "contam_indices <- which(asv_fasta %in% paste0(\">\", contam_asvs))\n",
    "dont_want <- sort(c(contam_indices, contam_indices + 1))\n",
    "asv_fasta_no_contam <- asv_fasta[- dont_want]\n",
    "\n",
    "# haciendo una nueva tabla de conteos\n",
    "asv_tab_no_contam <- asv_tab[!row.names(asv_tab) %in% contam_asvs, ]\n",
    "\n",
    "# realizando una nueva tabla de taxonomía\n",
    "asv_tax_no_contam <- asv_tax[!row.names(asv_tax) %in% contam_asvs, ]\n",
    "\n",
    "## y ahora escribiéndolo a nuevos archivos\n",
    "write(asv_fasta_no_contam, \"ASVs-no-contam.fa\")\n",
    "\n",
    "write.table(asv_tab_no_contam, \"ASVs_counts-no-contam.tsv\", sep=\"\\t\", quote=F, col.names=NA)\n",
    "write.table(asv_tax_no_contam, \"ASVs_taxonomy-no-contam.tsv\", sep=\"\\t\", quote=F, col.names=NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f29727",
   "metadata": {},
   "source": [
    "## 5. Calculating Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab0c68",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f68dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(tidyverse)\n",
    "library(phyloseq)\n",
    "library(vegan)\n",
    "library(DESeq2)\n",
    "library(dendextend)\n",
    "library(viridis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d926b42",
   "metadata": {},
   "source": [
    "Estas son las librerías necesarias. Ahora, vamos a alistar nuestras tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "write(asv_fasta_no_contam, \"ASVs-no-contam.fa\")\n",
    "write.table(asv_tab_no_contam, \"ASVs_counts-no-contam.tsv\", sep=\"\\t\", quote=F, col.names=NA)\n",
    "write.table(asv_tax_no_contam, \"ASVs_taxonomy-no-contam.tsv\", sep=\"\\t\", quote=F, col.names=NA)\n",
    "\n",
    "#ATENCION: Esto remueve todos tus archivos. Asegúrate de que tienes todo.\n",
    "\n",
    "rm(list=ls())\n",
    "count_tab <- read.table(\"ASVs_counts-no-contam.tsv\", header=T, row.names=1, check.names=F, sep=\"\\t\")[ , -c(1:4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a2170",
   "metadata": {},
   "source": [
    "Momento ahora de realizar nuestra tabla de metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e63e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "tax_tab <- as.matrix(read.table(\"ASVs_taxonomy-no-contam.tsv\", header=T, row.names=1, check.names=F, sep=\"\\t\"))\n",
    "sample_info_tab <- read.table(\"sample_info.tsv\", header=T, row.names=1, check.names=F, sep=\"\\t\")\n",
    "\n",
    "#Vamos a necesitar esta columna pronto\n",
    "\n",
    "sample_info_tab$color <- as.character(sample_info_tab$color)\n",
    "sample_info_tab # checa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5e74a",
   "metadata": {},
   "source": [
    "Ahora, con esto nos falta normalizar nuestra data. Para esto hay varias técnicas. Nosotros nos valdremos de un paquete que puede realizar esta función para nosotros sin necesidad de recurrir a técnicas misteriosas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "deseq_counts <- DESeqDataSetFromMatrix(count_tab, colData = sample_info_tab, design = ~type)\n",
    "deseq_counts_vst <- varianceStabilizingTransformation(deseq_counts)\n",
    "\n",
    "# ATENCIÓN por si figura un error:\n",
    "# deseq_counts <- estimateSizeFactors(deseq_counts, type = \"poscounts\")\n",
    "# deseq_counts_vst <- varianceStabilizingTransformation(deseq_counts)\n",
    "\n",
    "vst_trans_count_tab <- assay(deseq_counts_vst)\n",
    "euc_dist <- dist(t(vst_trans_count_tab))\n",
    "#Esto permite calcular la distancia euclidiana que requerimos para nuestra primera evaluación de beta-diversidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b893545",
   "metadata": {},
   "source": [
    "### Beta-diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd22da",
   "metadata": {},
   "source": [
    "Ya con las distancias hechas, estamos listos para dibujar nuestra primera gráfica, que vendría a ser la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "euc_clust <- hclust(euc_dist, method=\"ward.D2\")\n",
    "\n",
    "#Se puede plotear de este modo\n",
    "plot(euc_clust)\n",
    "\n",
    "#Como también podemos transformarlo en un dendograma, que es más simple de editar\n",
    "euc_dend <- as.dendrogram(euc_clust, hang=0.1)\n",
    "dend_cols <- as.character(sample_info_tab$color[order.dendrogram(euc_dend)])\n",
    "labels_colors(euc_dend) <- dend_cols\n",
    "\n",
    "plot(euc_dend, ylab=\"VST Euc. dist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70c897",
   "metadata": {},
   "source": [
    "Y ahora, por qué no aplicar un PCoA. Este tipo de gráfica nos permitirá ver una posible clusterización. Esto lo podremos lograr de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff49fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#Phyloseq funciona otorgándole un objeto que se forma con las tablas\n",
    "#hechas previamente. Así que hay que ponerlas de la siguiente forma:\n",
    "vst_count_phy <- otu_table(vst_trans_count_tab, taxa_are_rows=T)\n",
    "sample_info_tab_phy <- sample_data(sample_info_tab)\n",
    "vst_physeq <- phyloseq(vst_count_phy, sample_info_tab_phy)\n",
    "\n",
    "# Generamos el PCoA de la siguiente forma.\n",
    "vst_pcoa <- ordinate(vst_physeq, method=\"MDS\", distance=\"euclidean\")\n",
    "eigen_vals <- vst_pcoa$values$Eigenvalues #escalamiento de ejes\n",
    "\n",
    "plot_ordination(vst_physeq, vst_pcoa, color=\"char\") \n",
    "+ geom_point(size=1) \n",
    "+ labs(col=\"type\") \n",
    "+ geom_text(aes(label=rownames(sample_info_tab), hjust=0.3, vjust=-0.4)) \n",
    "+ coord_fixed(sqrt(eigen_vals[2]/eigen_vals[1])) \n",
    "+ ggtitle(\"PCoA\") \n",
    "+ scale_color_manual(values=unique(sample_info_tab$color[order(sample_info_tab$char)])) \n",
    "+ theme_bw() \n",
    "+ theme(legend.position=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe61ebe",
   "metadata": {},
   "source": [
    "### Alfa-diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b30d9",
   "metadata": {},
   "source": [
    "Y ahora, pasaremos a ver la alfa diversidad de nuestras muestras. Aquí usaremos diversidad Shannon y Chao1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08917491",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#first we need to create a phyloseq object using our un-transformed count table\n",
    "\n",
    "count_tab_phy <- otu_table(count_tab, taxa_are_rows=T)\n",
    "tax_tab_phy <- tax_table(tax_tab)\n",
    "\n",
    "ASV_physeq <- phyloseq(count_tab_phy, tax_tab_phy, sample_info_tab_phy)\n",
    "\n",
    "#and now we can call the plot_richness() function on our phyloseq object\n",
    "\n",
    "plot_richness(ASV_physeq, color=\"char\", measures=c(\"Chao1\", \"Shannon\")) \n",
    "+ scale_color_manual(values=unique(sample_info_tab$color[order(sample_info_tab$char)])) \n",
    "+ theme_bw() + theme(legend.title = element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf0fd3",
   "metadata": {},
   "source": [
    "También podemos tener otra forma de presentación para representar la riqueza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "plot_richness(ASV_physeq, x=\"type\", color=\"char\", measures=c(\"Chao1\", \"Shannon\")) \n",
    "+ scale_color_manual(values=unique(sample_info_tab$color[order(sample_info_tab$char)])) \n",
    "+ theme_bw() + theme(legend.title = element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc48a5",
   "metadata": {},
   "source": [
    "## 5. Visualizing the taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5096cc",
   "metadata": {},
   "source": [
    "Ahora, podemos visualizar los grupos taxonómicos que hemos obtenido. Para ello, habrá que alistar los grupos taxonómicos que tenemos.\n",
    "\n",
    "Haremos una tabla que sume a los phylums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "phyla_counts_tab <- otu_table(tax_glom(ASV_physeq, taxrank=\"phylum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8781055",
   "metadata": {},
   "source": [
    "Un vector que pueda darnos los nombres de filas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "phyla_tax_vec <- as.vector(tax_table(tax_glom(ASV_physeq, taxrank=\"phylum\"))[,\"phylum\"])\n",
    "rownames(phyla_counts_tab) <- as.vector(phyla_tax_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddda06c",
   "metadata": {},
   "source": [
    "Es momento de ordenar y tomar en cuenta a los no-clasificados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "unclassified_tax_counts <- colSums(count_tab) -\n",
    "colSums(phyla_counts_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3a600",
   "metadata": {},
   "source": [
    "Y ahora habrá que añadir esto a la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32003d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "phyla_and_unidentified_counts_tab <- rbind(phyla_counts_tab, \"Unclassified\"=unclassified_tax_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98747f01",
   "metadata": {},
   "source": [
    "Ahora nos deshacemos de *Proteobacteria*. No worries, lo agregaremos después cuando hagamos algo por clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "temp_major_taxa_counts_tab <- phyla_and_unidentified_counts_tab[!row.names(phyla_and_unidentified_counts_tab) %in% \"Proteobacteria\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652df42",
   "metadata": {},
   "source": [
    "Y ahora haremos una tabla por clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "class_counts_tab <- otu_table(tax_glom(ASV_physeq, taxrank=\"class\"))\n",
    "\n",
    "phy_tmp_vec <- class_tax_phy_tab[,2]\n",
    "class_tmp_vec <- class_tax_phy_tab[,3]\n",
    "rows_tmp <- row.names(class_tax_phy_tab)\n",
    "class_tax_tab <- data.frame(\"phylum\"=phy_tmp_vec, \"class\"=class_tmp_vec, row.names = rows_tmp)\n",
    "\n",
    "# Un vector que tenga todas las clases de Proteobacteria\n",
    "proteo_classes_vec <- as.vector(class_tax_tab[class_tax_tab$phylum == \"Proteobacteria\", \"class\"])\n",
    "\n",
    "# Cambiando nombres\n",
    "rownames(class_counts_tab) <- as.vector(class_tax_tab$class)\n",
    "\n",
    "# Conteo de clases de proteobacterias\n",
    "proteo_class_counts_tab <- class_counts_tab[row.names(class_counts_tab) %in% proteo_classes_vec, ]\n",
    "\n",
    "#Aquellas que no clasificaron, hay que ponerlas en algún lado\n",
    "proteo_no_class_annotated_counts <- phyla_and_unidentified_counts_tab[row.names(phyla_and_unidentified_counts_tab) %in% \"Proteobacteria\", ] - colSums(proteo_class_counts_tab)\n",
    "\n",
    "#Ahora los combinaremos\n",
    "major_taxa_counts_tab <- rbind(temp_major_taxa_counts_tab, proteo_class_counts_tab, \"Unresolved_Proteobacteria\"=proteo_no_class_annotated_counts)\n",
    "\n",
    "# Para ver que no hayamos perdido info, comparamos. Si es TRUE, bien\n",
    "identical(colSums(major_taxa_counts_tab), colSums(count_tab))\n",
    "\n",
    "# tabla de proporciones\n",
    "major_taxa_proportions_tab <- apply(major_taxa_counts_tab, 2, function(x) x/sum(x)*100)\n",
    "\n",
    "# Chequeemos la dimensión\n",
    "dim(major_taxa_proportions_tab)\n",
    "\n",
    "# Filtremos aquellos grupos con menos del 5%\n",
    "temp_filt_major_taxa_proportions_tab <-\n",
    "data.frame(major_taxa_proportions_tab[apply(major_taxa_proportions_tab, 1, max) > 5, ])\n",
    "\n",
    "# Después del filtrado\n",
    "dim(temp_filt_major_taxa_proportions_tab)\n",
    "\n",
    "#Creamos el grupo “other” para no perder la cabeza\n",
    "filtered_proportions <- colSums(major_taxa_proportions_tab) -\n",
    "colSums(temp_filt_major_taxa_proportions_tab)\n",
    "filt_major_taxa_proportions_tab <- rbind(temp_filt_major_taxa_proportions_tab, \"Other\"=filtered_proportions)\n",
    "\n",
    "# copia para editar sin remordimiento\n",
    "filt_major_taxa_proportions_tab_for_plot <- filt_major_taxa_proportions_tab\n",
    "\n",
    "#columna de nombres de taxa\n",
    "filt_major_taxa_proportions_tab_for_plot$Major_Taxa <- row.names(filt_major_taxa_proportions_tab_for_plot)\n",
    "\n",
    "# transformamos la tabla para que sea más fácil de graficar\n",
    "filt_major_taxa_proportions_tab_for_plot.g <- pivot_longer(filt_major_taxa_proportions_tab_for_plot, !Major_Taxa, names_to = \"Sample\", values_to = \"Proportion\") %>% data.frame()\n",
    "\n",
    "# Añadimos columnas de color y características\n",
    "sample_info_for_merge<-data.frame(\"Sample\"=row.names(sample_info_tab), \"char\"=sample_info_tab$char, \"color\"=sample_info_tab$color, stringsAsFactors=F)\n",
    "\n",
    "#Añadimos esto a nuestra tabla hecha para graficar\n",
    "filt_major_taxa_proportions_tab_for_plot.g2 <- merge(filt_major_taxa_proportions_tab_for_plot.g, sample_info_for_merge)\n",
    "\n",
    "# Y ahora ggplot\n",
    "ggplot(filt_major_taxa_proportions_tab_for_plot.g2, aes(x=Sample, y=Proportion, fill=Major_Taxa)) +\n",
    "geom_bar(width=0.6, stat=\"identity\") +\n",
    "theme_bw() +\n",
    "theme(axis.text.x=element_text(angle=90, vjust=0.4, hjust=1), legend.title=element_blank()) +\n",
    "labs(x=\"Sample\", y=\"% of 16S rRNA gene copies recovered\", title=\"All samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1ab2f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dada2_env",
   "language": "python",
   "name": "dada2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
